{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Machine Learning 2\n",
    "## Course project          \n",
    "                                                 Author: Diego Rodriguez\n",
    "## Dense network\n",
    "Finally, try with neural networks\n",
    "- 1-layer dense network i.e. no hidden layer, just the input and output ones \n",
    "- 2-layer dense network i.e. one hidden layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_features', 'validation_features', 'test_features', 'train_labels', 'validation_labels', 'test_labels', 'train_pixels', 'validation_pixels', 'test_pixels']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the npz file\n",
    "base_dir = '/Users/rodriguezmod/Downloads/swissroads/'\n",
    "\n",
    "with np.load(base_dir+'features.npz', allow_pickle=False) as npz_file: \n",
    "    # It's a dictionary-like object \n",
    "    print(list(npz_file.keys()))\n",
    "    \n",
    "    # Load the arrays    \n",
    "    # Merging test and validation features data to use a cross-validation approach to model fitting.\n",
    "    X_tr = np.concatenate((npz_file['train_features'], npz_file['validation_features']))\n",
    "    X_tr_pixels = np.concatenate((npz_file['train_pixels'], npz_file['validation_pixels']))\n",
    "    y_tr = np.concatenate((npz_file['train_labels'], npz_file['validation_labels']))\n",
    "    # Reduce to 1-dim\n",
    "    y_tr = np.argmax(y_tr, axis=1)\n",
    "\n",
    "    X_te = npz_file['test_features']\n",
    "    X_te_pixels = npz_file['test_pixels']\n",
    "    y_te = npz_file['test_labels']\n",
    "    # Reduce to 1-dim\n",
    "    y_te = np.argmax(y_te, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-layer\n",
    "1-layer dense network i.e. no hidden layer, just the input and output ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 216)               276696    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 1302      \n",
      "=================================================================\n",
      "Total params: 277,998\n",
      "Trainable params: 277,998\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Import warnings, there are a lot verbosity due deprecated tensorflow modules\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "# Create model with no hidden layers\n",
    "model_1l = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model_1l.add(Dense(\n",
    "    units=216, activation=activations.relu, input_dim=1280,\n",
    "    kernel_initializer=initializers.VarianceScaling(scale=2.0, seed=0)))\n",
    "\n",
    "# Output layer\n",
    "model_1l.add(Dense(\n",
    "    units=6, activation=activations.softmax,\n",
    "    kernel_initializer=initializers.VarianceScaling(scale=1.0, seed=0)))\n",
    "\n",
    "# Print network summary\n",
    "model_1l.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# Define loss function, optimizer and metrics to track during training\n",
    "model_1l.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 83 samples, validate on 336 samples\n",
      "Epoch 1/30\n",
      "83/83 [==============================] - 0s 4ms/sample - loss: 1.3271 - acc: 0.5663 - val_loss: 0.7623 - val_acc: 0.7619\n",
      "Epoch 2/30\n",
      "83/83 [==============================] - 0s 862us/sample - loss: 0.3880 - acc: 0.8795 - val_loss: 0.3875 - val_acc: 0.8601\n",
      "Epoch 3/30\n",
      "83/83 [==============================] - 0s 835us/sample - loss: 0.1022 - acc: 0.9639 - val_loss: 0.3363 - val_acc: 0.8839\n",
      "Epoch 4/30\n",
      "83/83 [==============================] - 0s 917us/sample - loss: 0.0467 - acc: 0.9880 - val_loss: 0.3053 - val_acc: 0.8839\n",
      "Epoch 5/30\n",
      "83/83 [==============================] - 0s 923us/sample - loss: 0.0202 - acc: 1.0000 - val_loss: 0.2783 - val_acc: 0.8810\n",
      "Epoch 6/30\n",
      "83/83 [==============================] - 0s 828us/sample - loss: 0.0189 - acc: 1.0000 - val_loss: 0.2984 - val_acc: 0.8929\n",
      "Epoch 7/30\n",
      "83/83 [==============================] - 0s 829us/sample - loss: 0.0087 - acc: 1.0000 - val_loss: 0.3859 - val_acc: 0.8571\n",
      "Epoch 8/30\n",
      "83/83 [==============================] - 0s 905us/sample - loss: 0.0084 - acc: 1.0000 - val_loss: 0.3392 - val_acc: 0.8720\n",
      "Epoch 9/30\n",
      "83/83 [==============================] - 0s 847us/sample - loss: 0.0057 - acc: 1.0000 - val_loss: 0.2921 - val_acc: 0.8988\n",
      "Epoch 10/30\n",
      "83/83 [==============================] - 0s 838us/sample - loss: 0.0046 - acc: 1.0000 - val_loss: 0.2869 - val_acc: 0.9018\n",
      "Epoch 11/30\n",
      "83/83 [==============================] - 0s 827us/sample - loss: 0.0040 - acc: 1.0000 - val_loss: 0.2921 - val_acc: 0.8988\n",
      "Epoch 12/30\n",
      "83/83 [==============================] - 0s 871us/sample - loss: 0.0034 - acc: 1.0000 - val_loss: 0.2994 - val_acc: 0.8929\n",
      "Epoch 13/30\n",
      "83/83 [==============================] - 0s 825us/sample - loss: 0.0030 - acc: 1.0000 - val_loss: 0.3037 - val_acc: 0.8899\n",
      "Epoch 14/30\n",
      "83/83 [==============================] - 0s 825us/sample - loss: 0.0028 - acc: 1.0000 - val_loss: 0.3083 - val_acc: 0.8899\n",
      "Epoch 15/30\n",
      "83/83 [==============================] - 0s 865us/sample - loss: 0.0025 - acc: 1.0000 - val_loss: 0.3046 - val_acc: 0.8869\n",
      "Epoch 16/30\n",
      "83/83 [==============================] - 0s 853us/sample - loss: 0.0023 - acc: 1.0000 - val_loss: 0.3056 - val_acc: 0.8899\n",
      "Epoch 17/30\n",
      "83/83 [==============================] - 0s 834us/sample - loss: 0.0021 - acc: 1.0000 - val_loss: 0.3101 - val_acc: 0.8869\n",
      "Epoch 18/30\n",
      "83/83 [==============================] - 0s 860us/sample - loss: 0.0020 - acc: 1.0000 - val_loss: 0.3067 - val_acc: 0.8899\n",
      "Epoch 19/30\n",
      "83/83 [==============================] - 0s 913us/sample - loss: 0.0018 - acc: 1.0000 - val_loss: 0.3104 - val_acc: 0.8839\n",
      "Epoch 20/30\n",
      "83/83 [==============================] - 0s 824us/sample - loss: 0.0017 - acc: 1.0000 - val_loss: 0.3137 - val_acc: 0.8839\n",
      "Epoch 21/30\n",
      "83/83 [==============================] - 0s 837us/sample - loss: 0.0016 - acc: 1.0000 - val_loss: 0.3066 - val_acc: 0.8869\n",
      "Epoch 22/30\n",
      "83/83 [==============================] - 0s 849us/sample - loss: 0.0015 - acc: 1.0000 - val_loss: 0.3067 - val_acc: 0.8899\n",
      "Epoch 23/30\n",
      "83/83 [==============================] - 0s 870us/sample - loss: 0.0014 - acc: 1.0000 - val_loss: 0.3102 - val_acc: 0.8839\n",
      "Epoch 24/30\n",
      "83/83 [==============================] - 0s 836us/sample - loss: 0.0013 - acc: 1.0000 - val_loss: 0.3132 - val_acc: 0.8839\n",
      "Epoch 25/30\n",
      "83/83 [==============================] - 0s 816us/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 0.3164 - val_acc: 0.8839\n",
      "Epoch 26/30\n",
      "83/83 [==============================] - 0s 888us/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 0.3169 - val_acc: 0.8839\n",
      "Epoch 27/30\n",
      "83/83 [==============================] - 0s 840us/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 0.3198 - val_acc: 0.8839\n",
      "Epoch 28/30\n",
      "83/83 [==============================] - 0s 809us/sample - loss: 0.0010 - acc: 1.0000 - val_loss: 0.3205 - val_acc: 0.8839\n",
      "Epoch 29/30\n",
      "83/83 [==============================] - 0s 834us/sample - loss: 9.7142e-04 - acc: 1.0000 - val_loss: 0.3195 - val_acc: 0.8869\n",
      "Epoch 30/30\n",
      "83/83 [==============================] - 0s 865us/sample - loss: 9.1977e-04 - acc: 1.0000 - val_loss: 0.3203 - val_acc: 0.8869\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "history_1l = model_1l.fit(\n",
    "    x=X_tr, y=y_tr,\n",
    "    validation_split=0.8, batch_size=10, epochs=30,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "419/419 [==============================] - 0s 102us/sample - loss: 0.2570 - acc: 0.9093\n",
      "50/50 [==============================] - 0s 151us/sample - loss: 0.4578 - acc: 0.8800\n",
      "Train loss: 0.26\n",
      "Train accuracy: 90.9%\n",
      "Test loss: 0.46\n",
      "Test accuracy: 88.0%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on train set\n",
    "(test_loss_tr, test_accuracy_tr) = model_1l.evaluate(X_tr, y_tr, batch_size=10)\n",
    "\n",
    "# Evaluate on test set\n",
    "(test_loss_te, test_accuracy_te) = model_1l.evaluate(X_te, y_te, batch_size=10)\n",
    "\n",
    "print('Train loss: {:.2f}'.format(test_loss_tr))\n",
    "print('Train accuracy: {:.1f}%'.format(100*test_accuracy_tr)) \n",
    "\n",
    "print('Test loss: {:.2f}'.format(test_loss_te))\n",
    "print('Test accuracy: {:.1f}%'.format(100*test_accuracy_te)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-layer\n",
    "2-layer dense network i.e. one hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 216)               276696    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 432)               93744     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 2598      \n",
      "=================================================================\n",
      "Total params: 373,038\n",
      "Trainable params: 373,038\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create model with one hidden layer\n",
    "model_2l = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model_2l.add(Dense(\n",
    "    units=216, activation=activations.relu, input_dim=1280,\n",
    "    kernel_initializer=initializers.VarianceScaling(scale=2.0, seed=0)))\n",
    "\n",
    "# One hidden layer\n",
    "model_2l.add(Dense(\n",
    "    units=216*2, activation=activations.relu,\n",
    "    kernel_initializer=initializers.VarianceScaling(scale=2.0, seed=0)))\n",
    "\n",
    "# Output layer\n",
    "model_2l.add(Dense(\n",
    "    units=6, activation=activations.softmax,\n",
    "    kernel_initializer=initializers.VarianceScaling(scale=1.0, seed=0)))\n",
    "\n",
    "# Print network summary\n",
    "model_2l.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# Define loss function, optimizer and metrics to track during training\n",
    "model_2l.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 83 samples, validate on 336 samples\n",
      "Epoch 1/30\n",
      "83/83 [==============================] - 0s 5ms/sample - loss: 1.2346 - acc: 0.5542 - val_loss: 0.5669 - val_acc: 0.8095\n",
      "Epoch 2/30\n",
      "83/83 [==============================] - 0s 1ms/sample - loss: 0.1830 - acc: 0.9639 - val_loss: 0.3277 - val_acc: 0.8512\n",
      "Epoch 3/30\n",
      "83/83 [==============================] - 0s 1ms/sample - loss: 0.0627 - acc: 0.9759 - val_loss: 0.3624 - val_acc: 0.8690\n",
      "Epoch 4/30\n",
      "83/83 [==============================] - 0s 1ms/sample - loss: 0.0154 - acc: 1.0000 - val_loss: 0.3831 - val_acc: 0.8542\n",
      "Epoch 5/30\n",
      "83/83 [==============================] - 0s 989us/sample - loss: 0.0077 - acc: 1.0000 - val_loss: 0.2965 - val_acc: 0.8929\n",
      "Epoch 6/30\n",
      "83/83 [==============================] - 0s 995us/sample - loss: 0.0028 - acc: 1.0000 - val_loss: 0.3160 - val_acc: 0.8958\n",
      "Epoch 7/30\n",
      "83/83 [==============================] - 0s 1ms/sample - loss: 0.0020 - acc: 1.0000 - val_loss: 0.3298 - val_acc: 0.8869\n",
      "Epoch 8/30\n",
      "83/83 [==============================] - 0s 1ms/sample - loss: 0.0014 - acc: 1.0000 - val_loss: 0.3285 - val_acc: 0.8869\n",
      "Epoch 9/30\n",
      "83/83 [==============================] - 0s 1ms/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 0.3336 - val_acc: 0.8899\n",
      "Epoch 10/30\n",
      "83/83 [==============================] - 0s 1ms/sample - loss: 8.8987e-04 - acc: 1.0000 - val_loss: 0.3373 - val_acc: 0.8899\n",
      "Epoch 11/30\n",
      "83/83 [==============================] - 0s 1ms/sample - loss: 7.8808e-04 - acc: 1.0000 - val_loss: 0.3413 - val_acc: 0.8869\n",
      "Epoch 12/30\n",
      "83/83 [==============================] - 0s 1ms/sample - loss: 6.9639e-04 - acc: 1.0000 - val_loss: 0.3513 - val_acc: 0.8839\n",
      "Epoch 13/30\n",
      "83/83 [==============================] - 0s 1ms/sample - loss: 6.2543e-04 - acc: 1.0000 - val_loss: 0.3598 - val_acc: 0.8839\n",
      "Epoch 14/30\n",
      "83/83 [==============================] - 0s 1ms/sample - loss: 5.6400e-04 - acc: 1.0000 - val_loss: 0.3646 - val_acc: 0.8839\n",
      "Epoch 15/30\n",
      "83/83 [==============================] - 0s 1ms/sample - loss: 5.0491e-04 - acc: 1.0000 - val_loss: 0.3624 - val_acc: 0.8869\n",
      "Epoch 16/30\n",
      "83/83 [==============================] - 0s 1ms/sample - loss: 4.4375e-04 - acc: 1.0000 - val_loss: 0.3570 - val_acc: 0.8839\n",
      "Epoch 17/30\n",
      "83/83 [==============================] - 0s 1ms/sample - loss: 4.0022e-04 - acc: 1.0000 - val_loss: 0.3566 - val_acc: 0.8839\n",
      "Epoch 18/30\n",
      "83/83 [==============================] - 0s 1ms/sample - loss: 3.6534e-04 - acc: 1.0000 - val_loss: 0.3612 - val_acc: 0.8839\n",
      "Epoch 19/30\n",
      "83/83 [==============================] - 0s 972us/sample - loss: 3.3353e-04 - acc: 1.0000 - val_loss: 0.3642 - val_acc: 0.8810\n",
      "Epoch 20/30\n",
      "83/83 [==============================] - 0s 1ms/sample - loss: 3.0242e-04 - acc: 1.0000 - val_loss: 0.3710 - val_acc: 0.8810\n",
      "Epoch 21/30\n",
      "83/83 [==============================] - 0s 1ms/sample - loss: 2.7669e-04 - acc: 1.0000 - val_loss: 0.3750 - val_acc: 0.8839\n",
      "Epoch 22/30\n",
      "83/83 [==============================] - 0s 1ms/sample - loss: 2.5512e-04 - acc: 1.0000 - val_loss: 0.3747 - val_acc: 0.8780\n",
      "Epoch 23/30\n",
      "83/83 [==============================] - 0s 1ms/sample - loss: 2.3386e-04 - acc: 1.0000 - val_loss: 0.3734 - val_acc: 0.8780\n",
      "Epoch 24/30\n",
      "83/83 [==============================] - 0s 985us/sample - loss: 2.1761e-04 - acc: 1.0000 - val_loss: 0.3747 - val_acc: 0.8780\n",
      "Epoch 25/30\n",
      "83/83 [==============================] - 0s 1ms/sample - loss: 2.0192e-04 - acc: 1.0000 - val_loss: 0.3799 - val_acc: 0.8780\n",
      "Epoch 26/30\n",
      "83/83 [==============================] - 0s 1ms/sample - loss: 1.8527e-04 - acc: 1.0000 - val_loss: 0.3801 - val_acc: 0.8780\n",
      "Epoch 27/30\n",
      "83/83 [==============================] - 0s 1ms/sample - loss: 1.7138e-04 - acc: 1.0000 - val_loss: 0.3823 - val_acc: 0.8780\n",
      "Epoch 28/30\n",
      "83/83 [==============================] - 0s 1ms/sample - loss: 1.5928e-04 - acc: 1.0000 - val_loss: 0.3876 - val_acc: 0.8780\n",
      "Epoch 29/30\n",
      "83/83 [==============================] - 0s 1ms/sample - loss: 1.4772e-04 - acc: 1.0000 - val_loss: 0.3880 - val_acc: 0.8780\n",
      "Epoch 30/30\n",
      "83/83 [==============================] - 0s 1ms/sample - loss: 1.3965e-04 - acc: 1.0000 - val_loss: 0.3873 - val_acc: 0.8780\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "history_2l = model_2l.fit(\n",
    "    x=X_tr, y=y_tr,\n",
    "    validation_split=0.8, batch_size=10, epochs=30,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "419/419 [==============================] - 0s 152us/sample - loss: 0.3106 - acc: 0.9021\n",
      "50/50 [==============================] - 0s 185us/sample - loss: 0.3621 - acc: 0.9000\n",
      "Train loss: 0.31\n",
      "Train accuracy: 90.2%\n",
      "Test loss: 0.36\n",
      "Test accuracy: 90.0%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on train set\n",
    "(test_loss_tr, test_accuracy_tr) = model_2l.evaluate(X_tr, y_tr, batch_size=10)\n",
    "\n",
    "# Evaluate on test set\n",
    "(test_loss_te, test_accuracy_te) = model_2l.evaluate(X_te, y_te, batch_size=10)\n",
    "\n",
    "print('Train loss: {:.2f}'.format(test_loss_tr))\n",
    "print('Train accuracy: {:.1f}%'.format(100*test_accuracy_tr)) \n",
    "\n",
    "print('Test loss: {:.2f}'.format(test_loss_te))\n",
    "print('Test accuracy: {:.1f}%'.format(100*test_accuracy_te)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting 1-layer dense network (no hidden layer) model an accuracy value of **88.0%** is obtained, which is a good value. With a 2-layer dense network (one hidden layer) model, an accuracy value of **90.0%** is obtained, it is a really good value, it is a bit better than the one obtained previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

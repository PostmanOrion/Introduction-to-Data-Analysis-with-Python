{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Machine Learning 2\n",
    "## Course project          \n",
    "                                                 Author: Diego Rodriguez\n",
    "\n",
    "## Feature extraction\n",
    "\n",
    "In this first part of the project, start by extracting a set of high-level features for each image in the data set. To achieve this, you can use ex. the Inception v3 or MobileNet v2 ConvNets which respectively extract 2048 and 1280 high-level features.\n",
    "\n",
    "This high-level features should then be used for all of the tasks in this project, except for when it is stated otherwise. In other words, the PCA exploration and all models (except for the Convolutional Neural Network) should use these high-level features. And in the case where we ask you to visualize the images, we of course mean to visualize the raw images with their pixel values.\n",
    "\n",
    "Suggestion: consider storing the extracted high-level features, e.g. in npz files, for quickly reloading them into each of the following notebooks.\n",
    "\n",
    "Note: All your models should be trained on the training set, and the fine tuning of your hyperparameters should be validated on the validation set. The final test set should only be used for the final comparison to test the accuracies of your models on a new dataset. However, in the case where you use a cross-validation approach, you can of course merge the train and validation set into one bigger dataset and use this for model fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import warnings, there are a lot verbosity due deprecated tensorflow modules\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np    \n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create graph. Code from course Applied Machine Learning 2 (EPFL Extension School)\n",
    "img_graph = tf.Graph()\n",
    "\n",
    "with img_graph.as_default():\n",
    "    \n",
    "    # Download module MobileNet v2\n",
    "    module_url = 'https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/2'\n",
    "    feature_extractor = hub.Module(module_url)\n",
    "\n",
    "    # Create input placeholder\n",
    "    input_imgs = tf.placeholder(dtype=tf.float32, shape=[None, 224, 224, 3])\n",
    "\n",
    "    # A node with the features\n",
    "    imgs_features = feature_extractor(input_imgs)\n",
    "\n",
    "    # Collect initializers\n",
    "    init_op = tf.group([\n",
    "        tf.global_variables_initializer(), tf.tables_initializer()\n",
    "    ])\n",
    "\n",
    "img_graph.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a session\n",
    "sess = tf.Session(graph=img_graph)\n",
    "\n",
    "# Initialize it\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 280 images belonging to 6 classes.\n",
      "Found 139 images belonging to 6 classes.\n",
      "Found 50 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "# Path to general directory with data\n",
    "base_dir = '/Users/rodriguezmod/Downloads/swissroads/'\n",
    "\n",
    "# Three paths for train, validation, and test data\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'valid')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "data_gen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# Extract features function\n",
    "def extract_features(data_dir, img_height, img_width, batch_size):\n",
    "    data_generator = data_gen.flow_from_directory(\n",
    "                            data_dir,\n",
    "                            target_size=(img_height, img_width),\n",
    "                            batch_size=batch_size,\n",
    "                            color_mode='rgb')\n",
    "    batch_index = 0\n",
    "    features = []\n",
    "    labels = []\n",
    "    pixels = []\n",
    "   \n",
    "    while batch_index <= data_generator.batch_index:\n",
    "        data = data_generator.next()\n",
    "        features_batch = sess.run(imgs_features, feed_dict={input_imgs: data[0]})\n",
    "        features[batch_index * batch_size : (batch_index + 1) * batch_size] = features_batch\n",
    "        labels[batch_index * batch_size : (batch_index + 1) * batch_size] = data[1]\n",
    "        pixels[batch_index * batch_size : (batch_index + 1) * batch_size] = data[0]\n",
    "        batch_index += 1\n",
    "\n",
    "    # Data_array is the numeric data of whole images\n",
    "    data_array_features = np.asarray(features)\n",
    "    data_array_labels = np.asarray(labels)\n",
    "    return data_array_features, data_array_labels, pixels\n",
    "\n",
    "# Calling function to extrated features\n",
    "train_features, train_labels, train_pixels = extract_features(train_dir, 224, 224, 70)\n",
    "validation_features, validation_labels, validation_pixels = extract_features(validation_dir, 224, 224, 70)\n",
    "test_features, test_labels, test_pixels = extract_features(test_dir, 224, 224, 70)\n",
    "\n",
    "# Save features, labels, and pixels into a .npz file\n",
    "np.savez(base_dir+'features.npz', \n",
    "         train_features=train_features, validation_features=validation_features, test_features=test_features,\n",
    "         train_labels=train_labels, validation_labels=validation_labels, test_labels=test_labels,\n",
    "         train_pixels=train_pixels, validation_pixels=validation_pixels, test_pixels=test_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

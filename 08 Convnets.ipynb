{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Machine Learning 2\n",
    "## Course project          \n",
    "                                                 Author: Diego Rodriguez\n",
    "## Convolutional neural network\n",
    "You tested above different models with the set of high-level features extracted from a pretrained neural network. However, can you get similar results by creating a ConvNet from scratch and using the pixel values from the original images to train the model?\n",
    "- What accuracy can you achieve?\n",
    "- Can you get good results? - If not, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (280, 256, 256, 3) (280,)\n",
      "Valid: (139, 256, 256, 3) (139,)\n",
      "Test: (50, 256, 256, 3) (50,)\n"
     ]
    }
   ],
   "source": [
    "# Upload data as pixels (arrays)\n",
    "import imageio\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "# Container and variables definitions\n",
    "category = [\"van\",\"truck\",\"other\",\"motorcycle\",\"car\",\"bike\"]\n",
    "X_train = []\n",
    "y_train = []\n",
    "label = 0\n",
    "\n",
    "for i in category:\n",
    "    files = glob.glob(\"/Users/rodriguezmod/Downloads/swissroads/train/\" + i + \"/*.png\")\n",
    "    label = label + 1\n",
    "    for myFile in files:\n",
    "        X_train.append(imageio.imread(myFile))\n",
    "        y_train.append(label)\n",
    "\n",
    "X_train = np.array(X_train, dtype='float32')\n",
    "# Rescale pixel values between -0.5 and 0.5\n",
    "X_train = (X_train - 128) / 255\n",
    "y_train = np.array(y_train, dtype='float64')\n",
    "\n",
    "# Valid data\n",
    "X_valid = []\n",
    "y_valid = []\n",
    "label = 0\n",
    "\n",
    "for i in category:\n",
    "    files = glob.glob(\"/Users/rodriguezmod/Downloads/swissroads/valid/\" + i + \"/*.png\")\n",
    "    label = label + 1\n",
    "    for myFile in files:\n",
    "        X_valid.append(imageio.imread(myFile))\n",
    "        y_valid.append(label)\n",
    "\n",
    "X_valid = np.array(X_valid, dtype='float32') \n",
    "# Rescale pixel values between -0.5 and 0.5\n",
    "X_valid = (X_valid - 128) / 255\n",
    "y_valid = np.array(y_valid, dtype='float64')\n",
    "\n",
    "# Test data\n",
    "X_test = []\n",
    "y_test = []\n",
    "label = 0\n",
    "\n",
    "for i in category:\n",
    "    files = glob.glob(\"/Users/rodriguezmod/Downloads/swissroads/test/\" + i + \"/*.png\")\n",
    "    label = label + 1\n",
    "    for myFile in files:\n",
    "        X_test.append(imageio.imread(myFile))\n",
    "        y_test.append(label)\n",
    "\n",
    "X_test = np.array(X_test, dtype='float32')\n",
    "# Rescale pixel values between -0.5 and 0.5\n",
    "X_test = (X_test - 128) / 255\n",
    "y_test = np.array(y_test, dtype='float64')\n",
    "\n",
    "print('Train:', X_train.shape, y_train.shape)\n",
    "print('Valid:', X_valid.shape, y_valid.shape)\n",
    "print('Test:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch generator\n",
    "def get_batches(X, y, batch_size):\n",
    "    # Shuffle X,y\n",
    "    shuffled_idx = np.arange(len(y))\n",
    "    np.random.shuffle(shuffled_idx)\n",
    "    \n",
    "    # Enumerate indexes by steps of batch_size\n",
    "    # i: 0, b, 2b, 3b, 4b, .. where b is the batch size\n",
    "    for i in range(0, len(y), batch_size):\n",
    "        # Batch indexes\n",
    "        batch_idx = shuffled_idx[i:i+batch_size]\n",
    "        yield X[batch_idx], y[batch_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fd55cc8aef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fd55cc8aef0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7fd55cc87d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7fd55cc87d30>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fd55241d390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fd55241d390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7fd55cc87d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7fd55cc87d30>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fd55e56fd30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fd55e56fd30>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fd55cc87d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fd55cc87d30>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd5510ac588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd5510ac588>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd5510ac588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd5510ac588>>: AttributeError: module 'gast' has no attribute 'Index'\n"
     ]
    }
   ],
   "source": [
    "# Import warnings, there are a lot verbosity due deprecated tensorflow modules\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "# Create new graph\n",
    "graph = tf.Graph()\n",
    "\n",
    "# Create graph. Code from course Applied Machine Learning 2 (EPFL Extension School)\n",
    "with graph.as_default():\n",
    "    # Placeholders\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=[None, 256, 256, 3])\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "    #print('Input:', y.shape)\n",
    "    \n",
    "    # Convolutional layer (64 filters, 5x5, stride: 2)\n",
    "    conv1 = tf.layers.conv2d(\n",
    "        X, 64, (5, 5), (2, 2), 'SAME', # \"same\" padding\n",
    "        activation=tf.nn.relu, # ReLU\n",
    "        kernel_initializer=tf.truncated_normal_initializer(stddev=0.01, seed=0),\n",
    "        name='conv1'\n",
    "    )\n",
    "    #print('Convolutional layer:', conv1.shape)\n",
    "    \n",
    "    # Maxpool layer (2x2, stride: 2, \"same\" padding)\n",
    "    pool1 = tf.layers.max_pooling2d(conv1, (2, 2), (2, 2), 'SAME')\n",
    "    #print('Maxpool:', pool1.shape)\n",
    "    \n",
    "    # Convolutional layer (64 filters, 3x3, stride: 1)\n",
    "    conv2 = tf.layers.conv2d(\n",
    "        pool1, 64, (3, 3), (1, 1), 'SAME', # \"same\" padding\n",
    "        activation=tf.nn.relu, # ReLU\n",
    "        kernel_initializer=tf.truncated_normal_initializer(stddev=0.01, seed=0),\n",
    "        name='conv2'\n",
    "    )\n",
    "    #print('Convolutional layer:', conv2.shape)\n",
    "    \n",
    "    # Maxpool layer (2x2, stride: 2, \"same\" padding)\n",
    "    pool2 = tf.layers.max_pooling2d(conv2, (2, 2), (2, 2), 'SAME')\n",
    "    #print('Maxpool:', pool2.shape)\n",
    "    \n",
    "    # Flatten output\n",
    "    flat_output = tf.contrib.layers.flatten(pool2)\n",
    "    #print('Flatten:', flat_output.shape)\n",
    "    \n",
    "    # Dropout\n",
    "    training = tf.placeholder(dtype=tf.bool)\n",
    "    flat_output = tf.layers.dropout(flat_output, rate=0.5, seed=0, training=training)\n",
    "    #print('Dropout:', flat_output.shape)\n",
    "    \n",
    "    # Fully connected layer\n",
    "    fc1 = tf.layers.dense(\n",
    "        flat_output, 256, # 256 hidden units\n",
    "        activation=tf.nn.relu, # ReLU\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=0),\n",
    "        bias_initializer=tf.zeros_initializer()\n",
    "    )\n",
    "    #print('Fully-connected layer:', fc1.shape)\n",
    "    \n",
    "    # Output layer\n",
    "    logits = tf.layers.dense(\n",
    "        fc1, 7, # One output unit per category\n",
    "        activation=None, # No activation function\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=1, seed=0),\n",
    "        bias_initializer=tf.zeros_initializer()\n",
    "    )\n",
    "    #print('Output layer:', logits.shape)\n",
    "    \n",
    "    # Kernel of the 1st conv. layer\n",
    "    with tf.variable_scope('conv1', reuse=True):\n",
    "        conv_kernels = tf.get_variable('kernel')\n",
    "    \n",
    "    # Mean cross-entropy\n",
    "    mean_ce = tf.reduce_mean(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=y, logits=logits))\n",
    "    \n",
    "    # Adam optimizer\n",
    "    lr = tf.placeholder(dtype=tf.float32)\n",
    "    gd = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "\n",
    "    # Minimize cross-entropy\n",
    "    train_op = gd.minimize(mean_ce)\n",
    "    \n",
    "    # Compute predictions and accuracy\n",
    "    predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "    is_correct = tf.equal(y, predictions)\n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvNet performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - valid: 0.230 train: 0.222 (mean)\n",
      "Epoch 2 - valid: 0.338 train: 0.344 (mean)\n",
      "Epoch 3 - valid: 0.302 train: 0.434 (mean)\n",
      "Epoch 4 - valid: 0.417 train: 0.460 (mean)\n",
      "Epoch 5 - valid: 0.403 train: 0.535 (mean)\n",
      "Epoch 6 - valid: 0.388 train: 0.637 (mean)\n",
      "Epoch 7 - valid: 0.381 train: 0.697 (mean)\n",
      "Epoch 8 - valid: 0.338 train: 0.807 (mean)\n",
      "Epoch 9 - valid: 0.367 train: 0.785 (mean)\n",
      "Epoch 10 - valid: 0.360 train: 0.871 (mean)\n",
      "Epoch 11 - valid: 0.396 train: 0.903 (mean)\n",
      "Epoch 12 - valid: 0.331 train: 0.953 (mean)\n",
      "Epoch 13 - valid: 0.338 train: 0.972 (mean)\n",
      "Epoch 14 - valid: 0.396 train: 0.978 (mean)\n",
      "Epoch 15 - valid: 0.374 train: 0.978 (mean)\n",
      "Epoch 16 - valid: 0.374 train: 0.988 (mean)\n",
      "Epoch 17 - valid: 0.331 train: 0.975 (mean)\n",
      "Epoch 18 - valid: 0.324 train: 0.994 (mean)\n",
      "Epoch 19 - valid: 0.317 train: 0.967 (mean)\n",
      "Epoch 20 - valid: 0.367 train: 0.973 (mean)\n",
      "Epoch 21 - valid: 0.367 train: 0.948 (mean)\n",
      "Epoch 22 - valid: 0.345 train: 0.945 (mean)\n",
      "Epoch 23 - valid: 0.388 train: 0.947 (mean)\n",
      "Epoch 24 - valid: 0.381 train: 0.976 (mean)\n",
      "Epoch 25 - valid: 0.353 train: 0.984 (mean)\n",
      "Epoch 26 - valid: 0.360 train: 0.970 (mean)\n",
      "Epoch 27 - valid: 0.396 train: 0.991 (mean)\n",
      "Epoch 28 - valid: 0.381 train: 0.991 (mean)\n",
      "Epoch 29 - valid: 0.424 train: 0.989 (mean)\n",
      "Epoch 30 - valid: 0.360 train: 1.000 (mean)\n",
      "Epoch 31 - valid: 0.367 train: 1.000 (mean)\n",
      "Epoch 32 - valid: 0.367 train: 1.000 (mean)\n",
      "Epoch 33 - valid: 0.367 train: 0.997 (mean)\n",
      "Epoch 34 - valid: 0.374 train: 1.000 (mean)\n",
      "Epoch 35 - valid: 0.381 train: 1.000 (mean)\n",
      "Epoch 36 - valid: 0.396 train: 1.000 (mean)\n",
      "Epoch 37 - valid: 0.396 train: 1.000 (mean)\n",
      "Epoch 38 - valid: 0.388 train: 1.000 (mean)\n",
      "Epoch 39 - valid: 0.388 train: 1.000 (mean)\n",
      "Epoch 40 - valid: 0.396 train: 1.000 (mean)\n",
      "Epoch 41 - valid: 0.403 train: 1.000 (mean)\n",
      "Epoch 42 - valid: 0.396 train: 1.000 (mean)\n",
      "Epoch 43 - valid: 0.381 train: 1.000 (mean)\n",
      "Epoch 44 - valid: 0.388 train: 1.000 (mean)\n",
      "Epoch 45 - valid: 0.381 train: 1.000 (mean)\n",
      "Epoch 46 - valid: 0.381 train: 1.000 (mean)\n",
      "Epoch 47 - valid: 0.381 train: 1.000 (mean)\n",
      "Epoch 48 - valid: 0.360 train: 1.000 (mean)\n",
      "Epoch 49 - valid: 0.367 train: 1.000 (mean)\n",
      "Epoch 50 - valid: 0.360 train: 1.000 (mean)\n",
      "Epoch 51 - valid: 0.381 train: 1.000 (mean)\n",
      "Epoch 52 - valid: 0.388 train: 1.000 (mean)\n",
      "Epoch 53 - valid: 0.388 train: 1.000 (mean)\n",
      "Epoch 54 - valid: 0.381 train: 1.000 (mean)\n",
      "Epoch 55 - valid: 0.396 train: 1.000 (mean)\n",
      "Epoch 56 - valid: 0.403 train: 1.000 (mean)\n",
      "Epoch 57 - valid: 0.396 train: 1.000 (mean)\n",
      "Epoch 58 - valid: 0.388 train: 1.000 (mean)\n",
      "Epoch 59 - valid: 0.396 train: 1.000 (mean)\n",
      "Epoch 60 - valid: 0.388 train: 1.000 (mean)\n",
      "Epoch 61 - valid: 0.388 train: 1.000 (mean)\n",
      "Epoch 62 - valid: 0.388 train: 1.000 (mean)\n",
      "Epoch 63 - valid: 0.388 train: 1.000 (mean)\n",
      "Epoch 64 - valid: 0.388 train: 1.000 (mean)\n",
      "Epoch 65 - valid: 0.388 train: 1.000 (mean)\n",
      "Epoch 66 - valid: 0.388 train: 1.000 (mean)\n",
      "Epoch 67 - valid: 0.381 train: 1.000 (mean)\n",
      "Epoch 68 - valid: 0.388 train: 1.000 (mean)\n",
      "Epoch 69 - valid: 0.388 train: 1.000 (mean)\n",
      "Epoch 70 - valid: 0.374 train: 1.000 (mean)\n",
      "Epoch 71 - valid: 0.381 train: 1.000 (mean)\n",
      "Epoch 72 - valid: 0.381 train: 1.000 (mean)\n",
      "Epoch 73 - valid: 0.374 train: 1.000 (mean)\n",
      "Epoch 74 - valid: 0.374 train: 1.000 (mean)\n",
      "Epoch 75 - valid: 0.396 train: 1.000 (mean)\n",
      "Epoch 76 - valid: 0.396 train: 1.000 (mean)\n",
      "Epoch 77 - valid: 0.396 train: 1.000 (mean)\n",
      "Epoch 78 - valid: 0.403 train: 1.000 (mean)\n",
      "Epoch 79 - valid: 0.374 train: 1.000 (mean)\n",
      "Epoch 80 - valid: 0.388 train: 1.000 (mean)\n",
      "Epoch 81 - valid: 0.388 train: 1.000 (mean)\n",
      "Epoch 82 - valid: 0.388 train: 1.000 (mean)\n",
      "Epoch 83 - valid: 0.396 train: 1.000 (mean)\n",
      "Epoch 84 - valid: 0.388 train: 1.000 (mean)\n",
      "Epoch 85 - valid: 0.388 train: 1.000 (mean)\n",
      "Epoch 86 - valid: 0.403 train: 1.000 (mean)\n",
      "Epoch 87 - valid: 0.403 train: 1.000 (mean)\n",
      "Epoch 88 - valid: 0.403 train: 1.000 (mean)\n",
      "Epoch 89 - valid: 0.403 train: 1.000 (mean)\n",
      "Epoch 90 - valid: 0.396 train: 1.000 (mean)\n",
      "Epoch 91 - valid: 0.396 train: 1.000 (mean)\n",
      "Epoch 92 - valid: 0.396 train: 1.000 (mean)\n",
      "Epoch 93 - valid: 0.388 train: 1.000 (mean)\n",
      "Epoch 94 - valid: 0.388 train: 1.000 (mean)\n",
      "Epoch 95 - valid: 0.388 train: 1.000 (mean)\n",
      "Epoch 96 - valid: 0.381 train: 1.000 (mean)\n",
      "Epoch 97 - valid: 0.381 train: 1.000 (mean)\n",
      "Epoch 98 - valid: 0.381 train: 1.000 (mean)\n",
      "Epoch 99 - valid: 0.381 train: 1.000 (mean)\n",
      "Epoch 100 - valid: 0.381 train: 1.000 (mean)\n",
      "Test accuracy: 52.0%\n"
     ]
    }
   ],
   "source": [
    "# Validation accuracy\n",
    "valid_acc_values = []\n",
    "\n",
    "# Work with session. Code from course Applied Machine Learning 2 (EPFL Extension School)\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Set seed\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    # Train several epochs\n",
    "    for epoch in range(100):\n",
    "        # Accuracy values (train) after each batch\n",
    "        batch_acc = []\n",
    "        \n",
    "        for X_batch, y_batch in get_batches(X_train, y_train, 64):\n",
    "            # Run training and evaluate accuracy\n",
    "            _, acc_value = sess.run([train_op, accuracy], feed_dict={\n",
    "                X: X_batch,\n",
    "                y: y_batch,\n",
    "                lr: 0.001, # Learning rate\n",
    "                training: True\n",
    "            })\n",
    "            \n",
    "            # Save accuracy (current batch)\n",
    "            batch_acc.append(acc_value)\n",
    "\n",
    "        # Evaluate validation accuracy\n",
    "        valid_acc = sess.run(accuracy, feed_dict={\n",
    "            X: X_valid,\n",
    "            y: y_valid,\n",
    "            training: False\n",
    "        })\n",
    "        valid_acc_values.append(valid_acc)\n",
    "        \n",
    "        # Print progress\n",
    "        print('Epoch {} - valid: {:.3f} train: {:.3f} (mean)'.format(\n",
    "            epoch+1, valid_acc, np.mean(batch_acc)\n",
    "        ))\n",
    "        \n",
    "    # Get 1st conv. layer kernels\n",
    "    kernels = conv_kernels.eval()\n",
    "    \n",
    "    # Evaluate test accuracy\n",
    "    test_acc = sess.run(accuracy, feed_dict={\n",
    "        X: X_test,\n",
    "        y: y_test,\n",
    "            training: False\n",
    "    })\n",
    "    print('Test accuracy: {:.1f}%'.format(100*test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe results\n",
    "- What accuracy can you achieve? **52.0%**\n",
    "- Can you get good results? - If not, why?\n",
    "\n",
    "*In our particular case, that is, our database with five typical categories (van, truck, motorcycle, car, bike), the use of pre-trained models such as **\"MobileNet v2\"** helps the forecast results have much more accuracy values than those that use neural networks based on from the raw pixels.*\n",
    "\n",
    "The bad result observed using only pixels-based model, is somewhat expected. In part it can be explained because all pixels in the image have \"equal value\" in the sense of the essential characteristics of the image and the background can be confused. We could think that the low accuracy results are due to the fact that the training of our neural network model is done with the database we have (280 images). It is clear that we can expand our database for training using but it is still insufficient.\n",
    "\n",
    "The use of analysis with feature-based models gives better results for two main reasons, according to my point of view: the extraction of the shapes (lines, curves etc.) that characterize an image and the use of pre-trained models such as **\"MobileNet v2\"** (Transfer learning) where the number of images with which these models were trained, greatly exceeds our data. *Please, see figure and table in notebook* **09 Results.ipynb** \n",
    "\n",
    "For the improvement of a model with raw pixels, I would hope that with the increase of the data available for training and the increase of layers and connections, such a model can improve the results. The obvious data acquisition process would increase and it would take a greater computing power and time invested in the processing.\n",
    "\n",
    "### Side notes\n",
    "\n",
    ">*Transitioning image models from pixel-based to feature-based allows us to extract information from images and video at a high level, to detect, classify, and track objects, co-register images, or understand a real-world scene. Using collections of features, we can train computers to recognize objects, with user-specified or automatically determined features.*(1) \n",
    "\n",
    ">*Image explanations are useful for two groups of people: model builders and model stakeholders. For data scientists and ML engineers building models, explanations can help verify that our model is picking up on the right signals in an image. In an apparel classification model, for example, if the highlighted pixels show that the model is looking at unique characteristics of a piece of clothing, we can be more confident that it’s behaving correctly for a particular image. However, if the highlighted pixels are instead in the background of the image, the model might not be learning the right features from our training data. In this case, explanations can help us identify and correct for imbalances in our data.*(2)\n",
    "\n",
    ">*Let’s back up a second and review the basics at a super high level. A convolutional neural network model can be trained to categorize images based upon the stuff in the images, and we measure the performance of this image classifier with metrics like accuracy, precision, recall, and so forth. These models are so standard that you can basically download a pre-trained neural network model (e.g. inception-V4, VGG19, mobilenet, and so on). If the classes (things) you want to recognize are not in the list of stuff recognized by the pre-trained model, then you can usually retrain the pre-trained neural network model to recognize the new stuff, by using most of the weights and connections of the pre-trained model. That’s called transfer learning.\n",
    "So, basically, we have a bunch of models out there that take in images and barf out labels. But why should I trust these models? Let’s have a look at what information one of these models is using to make predictions. This methodology can also be used to test out many other models including custom models.*(3)\n",
    "\n",
    ">*Recently, pixel/voxel-based ML (PML) emerged in medical image processing/analysis, which use pixel/voxel values in images directly instead of features calculated from segmented objects as input information; thus, feature calculation or segmentation is not required. Because the PML can avoid errors caused by inaccurate feature calculation and segmentation which often occur for subtle or complex objects, the performance of the PML can potentially be higher for such objects than that of common classifiers (i.e., feature-based MLs). In this paper, PMLs are surveyed to make clear (a) classes of PMLs, (b) similarities and differences within (among) different PMLs and those between PMLs and feature-based MLs, (c) advantages and limitations of PMLs, and (d) their applications in medical imaging.*(4)\n",
    "\n",
    "## References\n",
    "\n",
    "(1) https://datamanagement.hms.harvard.edu/event/pixels-features-models-image-processing-computer-vision-and-machine-and-deep-learning\n",
    "\n",
    "(2) https://cloud.google.com/blog/products/ai-machine-learning/explaining-model-predictions-on-image-data\n",
    "\n",
    "(3) https://towardsdatascience.com/justifying-image-classification-what-pixels-were-used-to-decide-2962e7e7391f\n",
    "\n",
    "(4) https://www.researchgate.net/publication/223963336_Pixel-Based_Machine_Learning_in_Medical_Imaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
